{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KerasでU-Net  \n",
    "[U-NetでPascal VOC 2012の画像をSemantic Segmentationする (TensorFlow)](https://qiita.com/tktktks10/items/0f551aea27d2f62ef708)\n",
    "Kerasバージョンに変更する\n",
    "\n",
    "* keras == 2.0.4  \n",
    "* tensorflow == 1.15.0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers.convolutional import Conv2D, ZeroPadding2D, Conv2DTranspose\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import LeakyReLU, BatchNormalization, Activation, Dropout\n",
    "\n",
    "class UNet(object):\n",
    "    def __init__(self, input_channel_count, output_channel_count, first_layer_filter_count):\n",
    "        self.INPUT_IMAGE_SIZE = 256\n",
    "        self.CONCATENATE_AXIS = -1\n",
    "        self.CONV_FILTER_SIZE = 4\n",
    "        self.CONV_STRIDE = 2\n",
    "        self.CONV_PADDING = (1, 1)\n",
    "        self.DECONV_FILTER_SIZE = 2\n",
    "        self.DECONV_STRIDE = 2\n",
    "\n",
    "        # (256 x 256 x input_channel_count)\n",
    "        inputs = Input((self.INPUT_IMAGE_SIZE, self.INPUT_IMAGE_SIZE, input_channel_count))\n",
    "\n",
    "        # エンコーダーの作成\n",
    "        # (128 x 128 x N)\n",
    "        enc1 = ZeroPadding2D(self.CONV_PADDING)(inputs)\n",
    "        enc1 = Conv2D(first_layer_filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(enc1)\n",
    "\n",
    "        # (64 x 64 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        enc2 = self._add_encoding_layer(filter_count, enc1)\n",
    "\n",
    "        # (32 x 32 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        enc3 = self._add_encoding_layer(filter_count, enc2)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        filter_count = first_layer_filter_count*8\n",
    "        enc4 = self._add_encoding_layer(filter_count, enc3)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        enc5 = self._add_encoding_layer(filter_count, enc4)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        enc6 = self._add_encoding_layer(filter_count, enc5)\n",
    "\n",
    "        # (2 x 2 x 8N)\n",
    "        enc7 = self._add_encoding_layer(filter_count, enc6)\n",
    "\n",
    "        # (1 x 1 x 8N)\n",
    "        enc8 = self._add_encoding_layer(filter_count, enc7)\n",
    "\n",
    "        # デコーダーの作成\n",
    "        # (2 x 2 x 8N)\n",
    "        dec1 = self._add_decoding_layer(filter_count, True, enc8)\n",
    "        dec1 = concatenate([dec1, enc7], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (4 x 4 x 8N)\n",
    "        dec2 = self._add_decoding_layer(filter_count, True, dec1)\n",
    "        dec2 = concatenate([dec2, enc6], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (8 x 8 x 8N)\n",
    "        dec3 = self._add_decoding_layer(filter_count, True, dec2)\n",
    "        dec3 = concatenate([dec3, enc5], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (16 x 16 x 8N)\n",
    "        dec4 = self._add_decoding_layer(filter_count, False, dec3)\n",
    "        dec4 = concatenate([dec4, enc4], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (32 x 32 x 4N)\n",
    "        filter_count = first_layer_filter_count*4\n",
    "        dec5 = self._add_decoding_layer(filter_count, False, dec4)\n",
    "        dec5 = concatenate([dec5, enc3], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (64 x 64 x 2N)\n",
    "        filter_count = first_layer_filter_count*2\n",
    "        dec6 = self._add_decoding_layer(filter_count, False, dec5)\n",
    "        dec6 = concatenate([dec6, enc2], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (128 x 128 x N)\n",
    "        filter_count = first_layer_filter_count\n",
    "        dec7 = self._add_decoding_layer(filter_count, False, dec6)\n",
    "        dec7 = concatenate([dec7, enc1], axis=self.CONCATENATE_AXIS)\n",
    "\n",
    "        # (256 x 256 x output_channel_count)\n",
    "        dec8 = Activation(activation='relu')(dec7)\n",
    "        dec8 = Conv2DTranspose(output_channel_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE)(dec8)\n",
    "        dec8 = Activation(activation='sigmoid')(dec8)\n",
    "\n",
    "        self.UNET = Model(input=inputs, output=dec8)\n",
    "\n",
    "    def _add_encoding_layer(self, filter_count, sequence):\n",
    "        new_sequence = LeakyReLU(0.2)(sequence)\n",
    "        new_sequence = ZeroPadding2D(self.CONV_PADDING)(new_sequence)\n",
    "        new_sequence = Conv2D(filter_count, self.CONV_FILTER_SIZE, strides=self.CONV_STRIDE)(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def _add_decoding_layer(self, filter_count, add_drop_layer, sequence):\n",
    "        new_sequence = Activation(activation='relu')(sequence)\n",
    "        new_sequence = Conv2DTranspose(filter_count, self.DECONV_FILTER_SIZE, strides=self.DECONV_STRIDE,\n",
    "                                       kernel_initializer='he_uniform')(new_sequence)\n",
    "        new_sequence = BatchNormalization()(new_sequence)\n",
    "        if add_drop_layer:\n",
    "            new_sequence = Dropout(0.5)(new_sequence)\n",
    "        return new_sequence\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.UNET\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理関連の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 値を-1から1に正規化する関数\n",
    "def normalize_x(image):\n",
    "    image = image/127.5 - 1\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から1に正規化する関数\n",
    "def normalize_y(image):\n",
    "    image = image/255\n",
    "    return image\n",
    "\n",
    "\n",
    "# 値を0から255に戻す関数\n",
    "def denormalize_y(image):\n",
    "    image = image*255\n",
    "    return image\n",
    "\n",
    "\n",
    "# インプット画像を読み込む関数\n",
    "def load_X(folder_path):\n",
    "    import os, cv2\n",
    "\n",
    "    image_files = os.listdir(folder_path)\n",
    "    image_files.sort()\n",
    "    images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 3), np.float32)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        images[i] = normalize_x(image)\n",
    "    return images, image_files\n",
    "\n",
    "\n",
    "# ラベル画像を読み込む関数\n",
    "def load_Y(folder_path):\n",
    "    import os, cv2\n",
    "\n",
    "    image_files = os.listdir(folder_path)\n",
    "    image_files.sort()\n",
    "    images = np.zeros((len(image_files), IMAGE_SIZE, IMAGE_SIZE, 1), np.float32)\n",
    "    for i, image_file in enumerate(image_files):\n",
    "        image = cv2.imread(folder_path + os.sep + image_file, cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        image = image[:, :, np.newaxis]\n",
    "        images[i] = normalize_y(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel_launcher.py:86: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2500/2500 [==============================] - 830s - loss: 0.4603 - dice_coef: 0.5397   \n",
      "Epoch 2/100\n",
      "2500/2500 [==============================] - 806s - loss: 0.3928 - dice_coef: 0.6072   \n",
      "Epoch 3/100\n",
      "2500/2500 [==============================] - 817s - loss: 0.3748 - dice_coef: 0.6252   \n",
      "Epoch 4/100\n",
      "1200/2500 [=============>................] - ETA: 435s - loss: 0.3594 - dice_coef: 0.6406"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[12,128,128,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradients_1/concatenate_14/concat_grad/Slice (defined at C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'gradients_1/concatenate_14/concat_grad/Slice':\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-200b219201df>\", line 65, in <module>\n    train_unet()\n  File \"<ipython-input-5-200b219201df>\", line 39, in train_unet\n    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\optimizers.py\", line 381, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py\", line 158, in gradients\n    unconnected_gradients)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 679, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 350, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 679, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\", line 228, in _ConcatGradV2\n    op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\", line 156, in _ConcatGradHelper\n    out_grads.append(array_ops.slice(grad, begin, size))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 855, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 9222, in _slice\n    \"Slice\", input=input, begin=begin, size=size, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'concatenate_14/concat', defined at:\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 26 identical lines from previous traceback]\n  File \"<ipython-input-5-200b219201df>\", line 65, in <module>\n    train_unet()\n  File \"<ipython-input-5-200b219201df>\", line 33, in train_unet\n    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n  File \"<ipython-input-2-a9958481612b>\", line 79, in __init__\n    dec7 = concatenate([dec7, enc1], axis=self.CONCATENATE_AXIS)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\layers\\merge.py\", line 508, in concatenate\n    return Concatenate(axis=axis, **kwargs)(inputs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\layers\\merge.py\", line 283, in call\n    return K.concatenate(inputs, axis=self.axis)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1681, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 1420, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 1257, in concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12,128,128,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradients_1/concatenate_14/concat_grad/Slice}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-200b219201df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m     \u001b[0mtrain_unet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-200b219201df>\u001b[0m in \u001b[0;36mtrain_unet\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mNUM_EPOCH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unet_weights.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1497\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2229\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                     \u001b[1;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[1;32m-> 1384\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[12,128,128,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node gradients_1/concatenate_14/concat_grad/Slice (defined at C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py:1748) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nOriginal stack trace for 'gradients_1/concatenate_14/concat_grad/Slice':\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 597, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\base_events.py\", line 541, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\base_events.py\", line 1786, in _run_once\n    handle._run()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2867, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3072, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-5-200b219201df>\", line 65, in <module>\n    train_unet()\n  File \"<ipython-input-5-200b219201df>\", line 39, in train_unet\n    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\", line 1481, in fit\n    self._make_train_function()\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\training.py\", line 1013, in _make_train_function\n    self.total_loss)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\optimizers.py\", line 381, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\optimizers.py\", line 47, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 2264, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_impl.py\", line 158, in gradients\n    unconnected_gradients)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 679, in _GradientsHelper\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 350, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gradients_util.py\", line 679, in <lambda>\n    lambda: grad_fn(op, *out_grads))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\", line 228, in _ConcatGradV2\n    op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_grad.py\", line 156, in _ConcatGradHelper\n    out_grads.append(array_ops.slice(grad, begin, size))\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 855, in slice\n    return gen_array_ops._slice(input_, begin, size, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 9222, in _slice\n    \"Slice\", input=input, begin=begin, size=size, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n\n...which was originally created as op 'concatenate_14/concat', defined at:\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 26 identical lines from previous traceback]\n  File \"<ipython-input-5-200b219201df>\", line 65, in <module>\n    train_unet()\n  File \"<ipython-input-5-200b219201df>\", line 33, in train_unet\n    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n  File \"<ipython-input-2-a9958481612b>\", line 79, in __init__\n    dec7 = concatenate([dec7, enc1], axis=self.CONCATENATE_AXIS)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\layers\\merge.py\", line 508, in concatenate\n    return Concatenate(axis=axis, **kwargs)(inputs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\engine\\topology.py\", line 585, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\layers\\merge.py\", line 283, in call\n    return K.concatenate(inputs, axis=self.axis)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 1681, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\", line 1420, in concat\n    return gen_array_ops.concat_v2(values=values, axis=axis, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\", line 1257, in concat_v2\n    \"ConcatV2\", values=values, axis=axis, name=name)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "#from unet import UNet\n",
    "\n",
    "# ダイス係数を計算する関数\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = K.flatten(y_true)\n",
    "    y_pred = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true * y_pred)\n",
    "    return 2.0 * intersection / (K.sum(y_true) + K.sum(y_pred) + 1)\n",
    "\n",
    "\n",
    "# ロス関数\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.0 - dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "# U-Netのトレーニングを実行する関数\n",
    "def train_unet():\n",
    "    X_train, file_names = load_X('trainingData' + os.sep + 'image')\n",
    "    Y_train = load_Y('trainingData' + os.sep + 'mask')\n",
    "\n",
    "    # 入力はBGR3チャンネル\n",
    "    input_channel_count = 3\n",
    "    # 出力はグレースケール1チャンネル\n",
    "    output_channel_count = 1\n",
    "    # 一番初めのConvolutionフィルタ枚数は64\n",
    "    first_layer_filter_count = 64\n",
    "    # U-Netの生成\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.compile(loss=dice_coef_loss, optimizer=Adam(), metrics=[dice_coef])\n",
    "\n",
    "    BATCH_SIZE = 12\n",
    "    NUM_EPOCH = 100\n",
    "    history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NUM_EPOCH, verbose=1)\n",
    "    model.save_weights('unet_weights.h5')\n",
    "\n",
    "\n",
    "# 学習後のU-Netによる予測を行う関数\n",
    "def predict():\n",
    "    import cv2\n",
    "\n",
    "    X_test, file_names = load_X('testData' + os.sep + 'image')\n",
    "\n",
    "    input_channel_count = 3\n",
    "    output_channel_count = 1\n",
    "    first_layer_filter_count = 64\n",
    "    network = UNet(input_channel_count, output_channel_count, first_layer_filter_count)\n",
    "    model = network.get_model()\n",
    "    model.load_weights('unet_weights.h5')\n",
    "    BATCH_SIZE = 12\n",
    "    Y_pred = model.predict(X_test, BATCH_SIZE)\n",
    "\n",
    "    for i, y in enumerate(Y_pred):\n",
    "        img = cv2.imread('testData' + os.sep + 'image' + os.sep + file_names[i])\n",
    "        y = cv2.resize(y, (img.shape[1], img.shape[0]))\n",
    "        cv2.imwrite('prediction' + os.sep + 'prediction' + str(i) + '.png', denormalize_y(y))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_unet()\n",
    "    predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u-net-keras",
   "language": "python",
   "name": "u-net-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
