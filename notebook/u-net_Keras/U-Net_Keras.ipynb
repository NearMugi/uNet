{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KerasでU-Net  \n",
    "[U-NetでPascal VOC 2012の画像をSemantic Segmentationする (TensorFlow)](https://qiita.com/tktktks10/items/0f551aea27d2f62ef708)\n",
    "Kerasバージョンに変更する\n",
    "\n",
    "* keras == 2.0.4  \n",
    "* tensorflow == 1.15.0  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from util import loader as ld\n",
    "\n",
    "\n",
    "class UNet:\n",
    "    def __init__(self, size=(128, 128), l2_reg=None):\n",
    "        self.model = self.create_model(size, l2_reg)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_model(size, l2_reg):\n",
    "        inputs = tf.placeholder(tf.float32, [None, size[0], size[1], 3])\n",
    "        teacher = tf.placeholder(tf.float32, [None, size[0], size[1], len(ld.DataSet.CATEGORY)])\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # 1, 1, 3\n",
    "        conv1_1 = UNet.conv(inputs, filters=64, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        conv1_2 = UNet.conv(conv1_1, filters=64, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        pool1 = UNet.pool(conv1_2)\n",
    "\n",
    "        # 1/2, 1/2, 64\n",
    "        conv2_1 = UNet.conv(pool1, filters=128, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        conv2_2 = UNet.conv(conv2_1, filters=128, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        pool2 = UNet.pool(conv2_2)\n",
    "\n",
    "        # 1/4, 1/4, 128\n",
    "        conv3_1 = UNet.conv(pool2, filters=256, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        conv3_2 = UNet.conv(conv3_1, filters=256, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        pool3 = UNet.pool(conv3_2)\n",
    "\n",
    "        # 1/8, 1/8, 256\n",
    "        conv4_1 = UNet.conv(pool3, filters=512, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        conv4_2 = UNet.conv(conv4_1, filters=512, l2_reg_scale=l2_reg, batchnorm_istraining=is_training)\n",
    "        pool4 = UNet.pool(conv4_2)\n",
    "\n",
    "        # 1/16, 1/16, 512\n",
    "        conv5_1 = UNet.conv(pool4, filters=1024, l2_reg_scale=l2_reg)\n",
    "        conv5_2 = UNet.conv(conv5_1, filters=1024, l2_reg_scale=l2_reg)\n",
    "        concated1 = tf.concat([UNet.conv_transpose(conv5_2, filters=512, l2_reg_scale=l2_reg), conv4_2], axis=3)\n",
    "\n",
    "        conv_up1_1 = UNet.conv(concated1, filters=512, l2_reg_scale=l2_reg)\n",
    "        conv_up1_2 = UNet.conv(conv_up1_1, filters=512, l2_reg_scale=l2_reg)\n",
    "        concated2 = tf.concat([UNet.conv_transpose(conv_up1_2, filters=256, l2_reg_scale=l2_reg), conv3_2], axis=3)\n",
    "\n",
    "        conv_up2_1 = UNet.conv(concated2, filters=256, l2_reg_scale=l2_reg)\n",
    "        conv_up2_2 = UNet.conv(conv_up2_1, filters=256, l2_reg_scale=l2_reg)\n",
    "        concated3 = tf.concat([UNet.conv_transpose(conv_up2_2, filters=128, l2_reg_scale=l2_reg), conv2_2], axis=3)\n",
    "\n",
    "        conv_up3_1 = UNet.conv(concated3, filters=128, l2_reg_scale=l2_reg)\n",
    "        conv_up3_2 = UNet.conv(conv_up3_1, filters=128, l2_reg_scale=l2_reg)\n",
    "        concated4 = tf.concat([UNet.conv_transpose(conv_up3_2, filters=64, l2_reg_scale=l2_reg), conv1_2], axis=3)\n",
    "\n",
    "        conv_up4_1 = UNet.conv(concated4, filters=64, l2_reg_scale=l2_reg)\n",
    "        conv_up4_2 = UNet.conv(conv_up4_1, filters=64, l2_reg_scale=l2_reg)\n",
    "        outputs = UNet.conv(conv_up4_2, filters=ld.DataSet.length_category(), kernel_size=[1, 1], activation=None)\n",
    "\n",
    "        return Model(inputs, outputs, teacher, is_training)\n",
    "\n",
    "    @staticmethod\n",
    "    def conv(inputs, filters, kernel_size=[3, 3], activation=tf.nn.relu, l2_reg_scale=None, batchnorm_istraining=None):\n",
    "        if l2_reg_scale is None:\n",
    "            regularizer = None\n",
    "        else:\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg_scale)\n",
    "        conved = tf.layers.conv2d(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=\"same\",\n",
    "            activation=activation,\n",
    "            kernel_regularizer=regularizer\n",
    "        )\n",
    "        if batchnorm_istraining is not None:\n",
    "            conved = UNet.bn(conved, batchnorm_istraining)\n",
    "\n",
    "        return conved\n",
    "\n",
    "    @staticmethod\n",
    "    def bn(inputs, is_training):\n",
    "        normalized = tf.layers.batch_normalization(\n",
    "            inputs=inputs,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=0.001,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            training=is_training,\n",
    "        )\n",
    "        return normalized\n",
    "\n",
    "    @staticmethod\n",
    "    def pool(inputs):\n",
    "        pooled = tf.layers.max_pooling2d(inputs=inputs, pool_size=[2, 2], strides=2)\n",
    "        return pooled\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_transpose(inputs, filters, l2_reg_scale=None):\n",
    "        if l2_reg_scale is None:\n",
    "            regularizer = None\n",
    "        else:\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=l2_reg_scale)\n",
    "        conved = tf.layers.conv2d_transpose(\n",
    "            inputs=inputs,\n",
    "            filters=filters,\n",
    "            strides=[2, 2],\n",
    "            kernel_size=[2, 2],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_regularizer=regularizer\n",
    "        )\n",
    "        return conved\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, inputs, outputs, teacher, is_training):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.teacher = teacher\n",
    "        self.is_training = is_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## メイン処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpu': 'store_true', 'epoch': 250, 'batchsize': 32, 'trainrate': 0.85, 'augmentation': 'store_true', 'l2reg': 0.0001}\n",
      "Loading original images.............. Completed\n",
      "Loading segmented images.............. Completed\n",
      "Casting to one-hot encoding... Done\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-1-c52e211d6860>:70: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Teppei\\Anaconda3\\envs\\u-net-keras\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-c52e211d6860>:86: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-1-c52e211d6860>:92: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-1-c52e211d6860>:108: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-e11582860611>:44: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from util import loader as ld\n",
    "from util import repoter as rp\n",
    "\n",
    "\n",
    "def load_dataset(train_rate):\n",
    "    loader = ld.Loader(\n",
    "        dir_original=\"data_set/VOCdevkit/VOC2012/JPEGImages\",\n",
    "        dir_segmented=\"data_set/VOCdevkit/VOC2012/SegmentationClass\",\n",
    "    )\n",
    "    return loader.load_train_test(train_rate=train_rate, shuffle=False)\n",
    "\n",
    "\n",
    "def train(parser):\n",
    "    # 訓練とテストデータを読み込みます\n",
    "    # Load train and test datas\n",
    "    train, test = load_dataset(train_rate=parser[\"trainrate\"])\n",
    "    valid = train.perm(0, 30)\n",
    "    test = test.perm(0, 150)\n",
    "\n",
    "    # 結果保存用のインスタンスを作成します\n",
    "    # Create Reporter Object\n",
    "    reporter = rp.Reporter(parser=parser)\n",
    "    accuracy_fig = reporter.create_figure(\n",
    "        \"Accuracy\", (\"epoch\", \"accuracy\"), [\"train\", \"test\"]\n",
    "    )\n",
    "    loss_fig = reporter.create_figure(\"Loss\", (\"epoch\", \"loss\"), [\"train\", \"test\"])\n",
    "\n",
    "    # GPUを使用するか\n",
    "    # Whether or not using a GPU\n",
    "    gpu = parser[\"gpu\"]\n",
    "\n",
    "    # モデルの生成\n",
    "    # Create a model\n",
    "    model_unet = UNet(l2_reg=parser[\"l2reg\"]).model\n",
    "\n",
    "    # 誤差関数とオプティマイザの設定をします\n",
    "    # Set a loss function and an optimizer\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=model_unet.teacher, logits=model_unet.outputs\n",
    "        )\n",
    "    )\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "    # 精度の算出をします\n",
    "    # Calculate accuracy\n",
    "    correct_prediction = tf.equal(\n",
    "        tf.argmax(model_unet.outputs, 3), tf.argmax(model_unet.teacher, 3)\n",
    "    )\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # セッションの初期化をします\n",
    "    # Initialize session\n",
    "    gpu_config = tf.ConfigProto(\n",
    "        gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7),\n",
    "        device_count={\"GPU\": 1},\n",
    "        log_device_placement=False,\n",
    "        allow_soft_placement=True,\n",
    "    )\n",
    "    sess = tf.InteractiveSession(config=gpu_config) if gpu else tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # 保存\n",
    "    # saver = tf.train.Saver()\n",
    "\n",
    "    # モデルの訓練\n",
    "    # Train the model\n",
    "    epochs = parser[\"epoch\"]\n",
    "    batch_size = parser[\"batchsize\"]\n",
    "    is_augment = parser[\"augmentation\"]\n",
    "    train_dict = {\n",
    "        model_unet.inputs: valid.images_original,\n",
    "        model_unet.teacher: valid.images_segmented,\n",
    "        model_unet.is_training: False,\n",
    "    }\n",
    "    test_dict = {\n",
    "        model_unet.inputs: test.images_original,\n",
    "        model_unet.teacher: test.images_segmented,\n",
    "        model_unet.is_training: False,\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train(batch_size=batch_size, augment=is_augment):\n",
    "            # バッチデータの展開\n",
    "            inputs = batch.images_original\n",
    "            teacher = batch.images_segmented\n",
    "            # Training\n",
    "            sess.run(\n",
    "                train_step,\n",
    "                feed_dict={\n",
    "                    model_unet.inputs: inputs,\n",
    "                    model_unet.teacher: teacher,\n",
    "                    model_unet.is_training: True,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # 評価\n",
    "        # Evaluation\n",
    "        if epoch % 1 == 0:\n",
    "            # saver.save(sess, \"/ckpt/model.ckpt\")\n",
    "            loss_train = sess.run(cross_entropy, feed_dict=train_dict)\n",
    "            loss_test = sess.run(cross_entropy, feed_dict=test_dict)\n",
    "            accuracy_train = sess.run(accuracy, feed_dict=train_dict)\n",
    "            accuracy_test = sess.run(accuracy, feed_dict=test_dict)\n",
    "            print(\"Epoch:\", epoch)\n",
    "            print(\"[Train] Loss:\", loss_train, \" Accuracy:\", accuracy_train)\n",
    "            print(\"[Test]  Loss:\", loss_test, \"Accuracy:\", accuracy_test)\n",
    "            accuracy_fig.add([accuracy_train, accuracy_test], is_update=True)\n",
    "            loss_fig.add([loss_train, loss_test], is_update=True)\n",
    "            if epoch % 3 == 0:\n",
    "                idx_train = random.randrange(10)\n",
    "                idx_test = random.randrange(100)\n",
    "                outputs_train = sess.run(\n",
    "                    model_unet.outputs,\n",
    "                    feed_dict={\n",
    "                        model_unet.inputs: [train.images_original[idx_train]],\n",
    "                        model_unet.is_training: False,\n",
    "                    },\n",
    "                )\n",
    "                outputs_test = sess.run(\n",
    "                    model_unet.outputs,\n",
    "                    feed_dict={\n",
    "                        model_unet.inputs: [test.images_original[idx_test]],\n",
    "                        model_unet.is_training: False,\n",
    "                    },\n",
    "                )\n",
    "                train_set = [\n",
    "                    train.images_original[idx_train],\n",
    "                    outputs_train[0],\n",
    "                    train.images_segmented[idx_train],\n",
    "                ]\n",
    "                test_set = [\n",
    "                    test.images_original[idx_test],\n",
    "                    outputs_test[0],\n",
    "                    test.images_segmented[idx_test],\n",
    "                ]\n",
    "                reporter.save_image_from_ndarray(\n",
    "                    train_set,\n",
    "                    test_set,\n",
    "                    train.palette,\n",
    "                    epoch,\n",
    "                    index_void=len(ld.DataSet.CATEGORY) - 1,\n",
    "                )\n",
    "\n",
    "    # 訓練済みモデルの評価\n",
    "    # Test the trained model\n",
    "    loss_test = sess.run(cross_entropy, feed_dict=test_dict)\n",
    "    accuracy_test = sess.run(accuracy, feed_dict=test_dict)\n",
    "    print(\"Result\")\n",
    "    print(\"[Test]  Loss:\", loss_test, \"Accuracy:\", accuracy_test)\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = {\n",
    "        \"gpu\": \"store_true\",\n",
    "        \"epoch\" : 250,\n",
    "        \"batchsize\" : 32,\n",
    "        \"trainrate\" : 0.85,\n",
    "        \"augmentation\" : \"store_true\",\n",
    "        \"l2reg\" : 0.0001\n",
    "    }\n",
    "    print(parser)\n",
    "    train(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u-net-keras",
   "language": "python",
   "name": "u-net-keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
